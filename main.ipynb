{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Notbook for `Any2Any-Transformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.__init__ import *\n",
    "from transformer.__init__ import *\n",
    "from training.__init__ import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positonal Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable()\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.length = 2048\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.d_model,\n",
    "            mask_zero=True\n",
    "        )\n",
    "        self.pos_encoding = self.positional_encoding()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[tf.newaxis, :tf.shape(x)[1], :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def positional_encoding(self):\n",
    "        depth = self.d_model / 2\n",
    "        depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "        angle_rads = np.arange(self.length)[:, np.newaxis] / (10000**depths)\n",
    "\n",
    "        pos_encoding = np.concatenate(\n",
    "            [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        super().get_config()\n",
    "        return {\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"d_model\": self.d_model\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Configurator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultBatchConfig():\n",
    "    def __init__(self, max_tokens, p_lang_tokenizer, s_lang_tokenizer):\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.p_lang_tokenizer = p_lang_tokenizer # primary language tokenizer\n",
    "        self.s_lang_tokenizer = s_lang_tokenizer # secondary language tokenizer\n",
    "\n",
    "    def __call__(self, p_lang, s_lang):\n",
    "        # tokenize the input\n",
    "        p_lang = self.p_lang_tokenizer.tokenize(p_lang)\n",
    "        s_lang = self.s_lang_tokenizer.tokenize(s_lang)\n",
    "\n",
    "        # only allow max number of tokens\n",
    "        p_lang = p_lang[:, :self.max_tokens]\n",
    "        s_lang = s_lang[:, :self.max_tokens +1] # +1 for end token\n",
    "\n",
    "        #\n",
    "        p_lang = p_lang.to_tensor()\n",
    "        s_lang_sentence = s_lang[:, :-1].to_tensor()\n",
    "\n",
    "        # label\n",
    "        s_lang_labels = s_lang[:, 1:].to_tensor()\n",
    "\n",
    "        return (p_lang, s_lang_sentence), s_lang_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator():\n",
    "    def __init__(self, max_tokens, p_lang_tokenizer, s_lang_tokenizer, tokenizer, transformer):\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.p_lang_tokenizer = p_lang_tokenizer\n",
    "        self.s_lang_tokenizer = s_lang_tokenizer\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, p_lang):\n",
    "        # init translation object\n",
    "        translation = Translation(\n",
    "            max_tokens=self.max_tokens,\n",
    "\n",
    "            p_lang_tokenizer=self.p_lang_tokenizer,\n",
    "            s_lang_tokenizer=self.s_lang_tokenizer,\n",
    "\n",
    "            tokenizer=self.tokenizer,\n",
    "            transformer=self.transformer\n",
    "        )\n",
    "\n",
    "        # translate the input\n",
    "        translation(\n",
    "            p_lang=tf.constant(p_lang),\n",
    "            s_lang_array=tf.TensorArray(\n",
    "                dtype=tf.int64,\n",
    "                size=0,\n",
    "                dynamic_size=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translation():\n",
    "    def __init__(self, max_tokens, p_lang_tokenizer, s_lang_tokenizer, tokenizer, transformer):\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.p_lang_tokenizer = p_lang_tokenizer\n",
    "        self.s_lang_tokenizer = s_lang_tokenizer\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, p_lang, s_lang_array):\n",
    "        # empty string handling\n",
    "        if len(p_lang.shape) == 0: p_lang = p_lang[tf.newaxis]\n",
    "\n",
    "        # tokenize\n",
    "        p_lang = self.p_lang_tokenizer.tokenize(p_lang).to_tensor()\n",
    "\n",
    "        # output start and end token\n",
    "        s_lang_start_token = self.s_lang_tokenizer.tokenize([\"\"])[0][0][tf.newaxis]\n",
    "        s_lang_end_token = self.s_lang_tokenizer.tokenize([\"\"])[0][1][tf.newaxis]\n",
    "\n",
    "        # write start token to output array\n",
    "        s_lang_array = s_lang_array.write(0, s_lang_start_token)\n",
    "\n",
    "        # writing the rest of the output\n",
    "        for i in tf.range(self.max_tokens):\n",
    "            # transpose array to tensor\n",
    "            s_lang = tf.transpose(s_lang_array.stack())\n",
    "\n",
    "            # prediciton\n",
    "            s_lang_token_predictions = self.transformer(\n",
    "                [p_lang, s_lang],\n",
    "                training=False\n",
    "            )\n",
    "            s_lang_token_prediction = tf.argmax(\n",
    "                s_lang_token_predictions[:, -1:, :],\n",
    "                axis=-1\n",
    "            )\n",
    "\n",
    "            # write predicted token to output array\n",
    "            s_lang_array = s_lang_array.write(i +1, s_lang_token_prediction[0])\n",
    "\n",
    "            # exiting if end token is last token\n",
    "            if s_lang_token_prediction == s_lang_end_token:\n",
    "                break\n",
    "\n",
    "        # transpose array to tensor\n",
    "        s_lang = tf.transpose(s_lang_array.stack())\n",
    "\n",
    "        # attention weights\n",
    "        self.transformer([p_lang, s_lang[:,:-1]], training=False)\n",
    "\n",
    "        # attributes\n",
    "        self.text = self.s_lang_tokenizer.detokenize(s_lang)[0]\n",
    "        self.tokens = self.s_lang_tokenizer.lookup(s_lang)[0]\n",
    "        self.weights = self.transformer.decoder.decoder_layers[-1].cross_attention.last_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset params\n",
    "MAX_TOKENS      = 128\n",
    "BATCH_SIZE      = 64\n",
    "BUFFER_SIZE     = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer hyperparams\n",
    "NUM_LAYERS      = 6\n",
    "D_MODEL         = 512\n",
    "NUM_HEADS       = 8\n",
    "DFF             = 2048\n",
    "DROPOUT_RATE    = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "WARMUP_STEPS    = 4000\n",
    "BETA_1          = 0.9\n",
    "BETA_2          = 0.98\n",
    "EPSILON         = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "EPOCHS          = 80\n",
    "STEPS_PER_EPOCH = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt-en dataset\n",
    "DATASET_NAME = \"ted_hrlr_translate/pt_to_en\"\n",
    "MODEL_NAME = \"ted_hrlr_translate_pt_en_converter\"\n",
    "\n",
    "dataset = DatasetHandler(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    model_name=MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    buffer_size=BUFFER_SIZE\n",
    ")\n",
    "\n",
    "# calling dataset content\n",
    "examples, metadata = dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle is that the Transformer is not actually configured.\n",
    "Rather it is class serving two different functions `call` and `train`.\n",
    "\n",
    "The `Encoder`s and `Decoder`s are the two models who are actually trained and altered.\n",
    "Therefore it is these two that we save and load into a static and unchanging transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt encoder\n",
    "encoder = Encoder(\n",
    "    encoder_name=\"pt-encoder\",\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DFF,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    entry_layer=tf.keras.saving.serialize_keras_object(\n",
    "        PositionalEmbedding(\n",
    "            vocab_size=dataset.tokenizer.pt.get_vocab_size().numpy(),\n",
    "            d_model=D_MODEL\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# en decoder\n",
    "decoder = Decoder(\n",
    "    decoder_name=\"en-decoder\",\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DFF,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    entry_layer=tf.keras.saving.serialize_keras_object(\n",
    "        PositionalEmbedding(\n",
    "            vocab_size=dataset.tokenizer.en.get_vocab_size().numpy(),\n",
    "            d_model=D_MODEL\n",
    "        )\n",
    "    ),\n",
    "    exit_layer=tf.keras.saving.serialize_keras_object(\n",
    "        tf.keras.layers.Dense(\n",
    "            dataset.tokenizer.en.get_vocab_size().numpy()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (pt, en), _ in dataset.make_batches(\n",
    "    examples[\"train\"],\n",
    "    batch_config=DefaultBatchConfig(\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        p_lang_tokenizer=dataset.tokenizer.pt,\n",
    "        s_lang_tokenizer=dataset.tokenizer.en\n",
    "    )\n",
    ").take(1):\n",
    "    break\n",
    "\n",
    "transformer.encoder = encoder\n",
    "transformer.decoder = decoder\n",
    "\n",
    "transformer((pt, en))\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the encoder model\n",
    "tf.keras.models.save_model(\n",
    "    model=encoder,\n",
    "    filepath=\"__models__/pt-test-encoder.keras\",\n",
    "    save_format=\"keras\"\n",
    ")\n",
    "\n",
    "# saving the decoder model\n",
    "tf.keras.models.save_model(\n",
    "    model=decoder,\n",
    "    filepath=\"__models__/en-test-decoder.keras\",\n",
    "    save_format=\"keras\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the encoder model from file\n",
    "encoder = tf.keras.models.load_model(\n",
    "    filepath=\"__models__/pt-test-encoder.keras\",\n",
    "    custom_objects={\n",
    "    }\n",
    ")\n",
    "\n",
    "# loading the decoder model from file\n",
    "decoder = tf.keras.models.load_model(\n",
    "    filepath=\"__models__/en-test-decoder.keras\",\n",
    "    custom_objects={\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init defaut batch configurator\n",
    "batch_config = DefaultBatchConfig(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "\n",
    "    p_lang_tokenizer=dataset.tokenizer.pt,\n",
    "    s_lang_tokenizer=dataset.tokenizer.en\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainging schedule\n",
    "trainging_schedule = [\n",
    "    TrainingScheduleElement(\n",
    "        encoder=encoder, decoder=decoder,\n",
    "\n",
    "        training_data=dataset.make_batches(\n",
    "            dataset=examples['train'],\n",
    "            batch_config=batch_config\n",
    "        ),\n",
    "        validation_data=dataset.make_batches(\n",
    "            dataset=examples['validation'],\n",
    "            batch_config=batch_config\n",
    "        ),\n",
    "\n",
    "        epochs=EPOCHS,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        callbacks=[\n",
    "            DefaultCallback(\n",
    "                path=\"__models__/__translator_v1__\",\n",
    "                model=transformer\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling for trainging\n",
    "transformer.compile(\n",
    "    loss=Loss.masked,\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=DefaultOptimizerSchedule(\n",
    "            d_model=D_MODEL,\n",
    "            warmup_steps=WARMUP_STEPS\n",
    "        ),\n",
    "        beta_1=BETA_1,\n",
    "        beta_2=BETA_2,\n",
    "        epsilon=EPSILON\n",
    "    ),\n",
    "    metrics=[Accuarcy.masked]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the training process\n",
    "transformer.train(\n",
    "    training_schedule=trainging_schedule,\n",
    "    callbacks=[   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = Translator(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "\n",
    "    p_lang_tokenizer=dataset.tokenizer.pt,\n",
    "    s_lang_tokenizer=dataset.tokenizer.en,\n",
    "\n",
    "    tokenizer=dataset.tokenizer,\n",
    "    transformer=transformer\n",
    ")\n",
    "\n",
    "sample_tranlation = translate(\n",
    "    p_lang=\"este é um problema que temos que resolver.\"\n",
    ")\n",
    "# >>> \"this is a problem we have to solve .\"\n",
    "\n",
    "print(sample_tranlation.text)\n",
    "print(sample_tranlation.tokens)\n",
    "print(sample_tranlation.weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "__venv__",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
